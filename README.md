# üéì Master-Seminar: AI for Vision-Language Models in Medical Imaging (IN2107, IN45069)

## Organizer: Chair of Computational Imaging and AI in Medicine (CompAI)
<p align="center">
    üë©‚Äçüè´ <strong><a href="https://compai-lab.github.io/author/julia-a.-schnabel/">Prof. Julia A. Schnabel</a></strong> &nbsp;&nbsp;‚îÇ
    üßë‚Äçüî¨ <strong><a href="https://cosmin-bercea.com/">Dr. Cosmin I. Bercea</a></strong> &nbsp;&nbsp;‚îÇ
    üë©‚Äçüíº <strong><a href="https://lijunrio.github.io/junli/">Jun Li</a></strong> &nbsp;&nbsp;‚îÇ
    üë©‚Äçüíº <strong><a href="https://compai-lab.github.io/author/ha-young-kim/">Ha Young Kim</a></strong>
</p>
<p align="center">
    üèõÔ∏è <strong><a href="https://compai-lab.github.io/">CompAI & IML Lab</a></strong>
</p>
<p align="center">
    <img src="images\compai.png" alt="Chair Members" width="800"/>
</p>

## üìò Seminar Overview

The **Master-Seminar on AI for Vision-Language Models in Medical Imaging** introduces students to the fundamentals of Vision-Language Models (VLMs) and their applications in the medical domain.

### Key Components:
- üìö **Student presentations** on the state-of-the-art (SOTA) VLM papers
- üßë‚Äçüè´ **Talks from invited researchers and professors**
- üõ†Ô∏è **Hands-on projects** exploring VLMs in real medical data

Our goal is to **foster curiosity**, **bridge modalities**, and **inspire new research** at the intersection of AI and medical imaging.


## üìñ Course Materials
 
All lecture PDFs are available in TUM Moodle .

| **Week** | **Topic**                           | **Slides**                                                                                         | **Recording**                                         |
|----------|-------------------------------------|----------------------------------------------------------------------------------------------------|------------------------------------------------------|
| Week 1   | Welcome and Introduction to VLMs-L1 | üìÑ [Course Requirements](./Materials/L1_25_VLM_Intro_30_04.pdf) <br> üìÑ [Introduction to VLMs](./Materials/VLM-L1.pdf) | ‚ñ∂Ô∏è [YouTube](https://www.youtube.com/watch?v=WDvDeD7nnP8) |
| Week 2   | How to Read Papers and Make Posters | üìÑ [Lecture-2](./Materials/07_05_2025_VLM_HowTo.pdf) <br>                                                                                    | *To be updated*                                       |\
...


## üìù Student Presentations

Each student is assigned a paper to present and implement a related project. GitHub links will be updated as projects are finalized.
- How to subbmit you final work: https://github.com/LijunRio/example_for_seminar

| **Student Name**          | **Paper Title**                                                                                  | **GitHub Repository**                 |
|---------------------------|------------------------------------------------------------------------------------------------|-------------------------------------|
| Jun Li ÔºàexampleÔºâ             | [Enhancing Abnormality Grounding for Vision-Language Models with Knowledge Descriptions](https://arxiv.org/pdf/2503.03278)  | [Repo](https://github.com/LijunRio/example_for_seminar) Here is you subbmision link                          |
| Bivek Panthi              | [Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning (27 Mar 2025)](https://arxiv.org/abs/2503.20752)  | [RepoBivek](https://github.com/panthibivek/AI-in-Vision-Language-Models-in-Medical-Imaging)                           |
| Luis Schmid               | [A Large Model for Non-Invasive and Personalized Management of Breast Cancer from MRI (17 Apr 2025)](https://arxiv.org/) | [RepoLuis](https://github.com/lelui/Final-Submission-In2107)                           |
| Radoslav M. Yankov        | [Qwen2.5-VL Technical Report (19 Feb 2025)](https://arxiv.org/)                                | [Repo](https://github.com/RadoslavMYankov/Medical-Imaging-Seminar)                           |
| Philipp R√∂ssel            | [Visual-RFT: Visual Reinforcement Fine-Tuning (3 Mar 2025)](https://arxiv.org/)                | [Repo](https://github.com/proessel/Seminar_VLM)                          |
| Dominik Garstenauer       | [MedRAX: Medical Reasoning Agent for Chest X-ray (4 Feb 2025)](https://arxiv.org/abs/2502.02673)             | [Repo](https://github.com/dominikg-tum/VLM-Qwen-Medical-Eval)                           |
| Jingyi He                 | [MAIRA-2: Grounded Radiology Report Generation (6 Jun 2024)](https://arxiv.org/)                |                            |
| Francesco Vaccaro         | [Detecting Hallucinations in Large Language Models Using Semantic Entropy (19 Jun 2024)](https://arxiv.org/) | [Repo](https://github.com/coding-novice/Final-Submission-VLM-SS25#)                           |
| Tuna Karacan              | [Premise Order Matters in Reasoning with Large Language Models](https://arxiv.org/abs/2402.08939) | [Repo](https://github.com/TunaKaracan/Final-Submission-In2107-Tuna)   
| Linfeng Guo               | [CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation (22 Jan 2024)](https://arxiv.org/) | [Repo](https://github.com/linfeng20001/Seminar-CheXagent-Towards-a-Foundation-Model-for-Chest-X-Ray-Interpretation-22-Jan-2024-.git) |
| Fatih Ibrahim √ñzl√ºgedik   |                                                                             |                          |
| Abdullah Utku Yertutan    |                                                                              |                           |

## üìö Recommand Paper List for the presentation

| #  | **Title**                                                                                          | **Time**     | **Link**                                               | **Topics**                             |
|----|----------------------------------------------------------------------------------------------------|--------------|--------------------------------------------------------|----------------------------------------|
| 1  | Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning                                          | 27 Mar 2025  | [Link](https://arxiv.org/abs/2503.20752)              | Visual Reasoning, Reinforcement Learning |
| 2  | LIMO: Less is More for Reasoning                                                                  | 5 Feb 2025   | [Link](https://arxiv.org/abs/2502.03387)              | Efficient Reasoning, LLMs              |
| 3  | Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases                          | 6 Mar 2025   | [Link](https://arxiv.org/abs/2503.04691)              | Clinical Reasoning, Evaluation         |
| 4  | Demystifying Long Chain-of-Thought Reasoning in LLMs                                              | 5 Feb 2025   | [Link](https://arxiv.org/abs/2502.03373)              | CoT Analysis, LLMs                     |
| 5  | Chain-of-Thought Prompting Elicits Reasoning in Large Language Models                             | 10 Jan 2023  | [Link](https://arxiv.org/abs/2201.11903)              | Prompt Engineering, Reasoning          |
| 6  | Visual-RFT: Visual Reinforcement Fine-Tuning                                                      | 3 Mar 2025   | [Link](https://arxiv.org/abs/2503.01785)              | Vision-Language, RL                    |
| 7  | RadVLM: A Multitask Conversational Vision-Language Model for Radiology                            | 18 Dec 2024  | [Link](https://arxiv.org/abs/2502.03333)              | Radiology, VLM                         |
| 8  | CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation                              | 22 Jan 2024  | [Link](https://arxiv.org/abs/2401.12208)              | Medical VLMs, X-ray                    |
| 9  | MAIRA-2: Grounded Radiology Report Generation                                                     | 6 Jun 2024   | [Link](https://arxiv.org/abs/2406.04449)              | Report Generation, Radiology           |
| 10 | Qwen2.5-VL Technical Report                                                                       | 19 Feb 2025  | [Link](https://arxiv.org/abs/2502.13923)              | Foundation Model, VLM                  |
| 11 | LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day              | 1 Jun 2023   | [Link](https://arxiv.org/abs/2306.00890)              | Medical VLMs                           |
| 12 | Towards Evaluating and Building Versatile Large Language Models for Medicine                     | 5 Sep 2024   | [Link](https://www.nature.com/articles/s41746-024-01390-4) | Medical LLMs, Generalist Models        |
| 13 | Can Modern LLMs Act as Agent Cores in Radiology Environments?                                      | 8 Apr 2025   | [Link](https://arxiv.org/pdf/2412.09529)              | Agents, Radiology                      |
| 14 | A Large Model for Non-Invasive and Personalized Management of Breast Cancer from MRI              | 17 Apr 2025  | [Link](https://www.nature.com/articles/s41467-025-58798-z) | Breast Cancer, MRI, Personalized AI    |
| 15 | MedRAX: Medical Reasoning Agent for Chest X-ray                                                   | 4 Feb 2025   | [Link](https://arxiv.org/abs/2502.02673)              | X-ray Reasoning, Agents                |
| 16 | MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for Medical Consultation               | 18 Mar 2025  | [Link](https://arxiv.org/abs/2503.13856)              | Multi-Agent, Healthcare AI             |
| 17 | Premise Order Matters in Reasoning with Large Language Models                                     | 14 Feb 2024  | [Link](https://arxiv.org/abs/2402.08939)              | Logical Reasoning, Prompt Order        |
| 18 | A Scalable Framework for Evaluating Health Language Models                                        | 30 Mar 2025  | [Link](https://arxiv.org/abs/2503.23339)              | Evaluation, Health LLMs                |
| 19 | Detecting hallucinations in large language models using semantic entropy                          | 19 Jun 2024  | [Link](https://www.nature.com/articles/s41586-024-07421-0) | LLM Evaluation, Hallucinations        |

> **Note**: If you are interested in a paper that is not included in the list above, feel free to email us about the paper, and we will be happy to include it!


## üíª Project Part 

### 1) ü©ª Datasets

Explore the dataset repository for the seminar:  
[üîó VLM-Seminar25 Dataset](https://github.com/LijunRio/VLM-Seminar25-Dataset)

### 2) üõ†Ô∏è Models to Use

#### Closed Source Models 
<details>
<summary>(Click to Expand)</summary>

- [ChatGPT Series](https://platform.openai.com): GPT-3, GPT-4, GPT-4o, ...
- [Gemini](https://aistudio.google.com): Gemini 2.5 Pro, Gemini 2.5 Flash, ...
- ...

</details>


#### Open Source Models 
<details>
<summary>(Click to Expand)</summary>

- [Baichuan 2](https://huggingface.co/baichuan-inc)  
- [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)  
- [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)  
- [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)  
- [Florence-2](https://huggingface.co/microsoft/Florence-2-large)  
- [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)  
- [Gemma 3](https://huggingface.co/google)  
- [GPT-2](https://huggingface.co/openai-community)  
- [Granite 3.0-3.3](https://huggingface.co/ibm-granite)  
- [InternLM 2-3](https://huggingface.co/internlm)  
- [InternVL 2.5-3](https://huggingface.co/OpenGVLab)  
- [Llama](https://github.com/facebookresearch/llama)  
- [Llama 2](https://huggingface.co/meta-llama)  
- [Llama 3-3.3](https://huggingface.co/meta-llama)  
- [Llama 4](https://huggingface.co/meta-llama)  
- [Llama 3.2 Vision](https://huggingface.co/meta-llama)  
- [LLaVA-1.5](https://huggingface.co/llava-hf)  
- [LLaVA-NeXT](https://huggingface.co/llava-hf)  
- [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)  
- [PaliGemma/PaliGemma2](https://huggingface.co/google)  
- [Phi-1.5/Phi-2](https://huggingface.co/microsoft)  
- [Phi-3/Phi-3.5](https://huggingface.co/microsoft)  
- [Phi-3-small](https://huggingface.co/microsoft)  
- [Phi-4](https://huggingface.co/microsoft)  
- [Pixtral](https://huggingface.co/mistralai)  
- [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)  
- [Qwen3 (MoE)](https://huggingface.co/Qwen)  
- [Qwen2-Audio](https://huggingface.co/Qwen)  
- [Qwen2.5-Omni](https://huggingface.co/Qwen)  
- [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)  

</details>


#### Medical Domain Models:
<details>
<summary>(Click to Expand)</summary>
- [ChexAgent](https://huggingface.co/StanfordAIMI/CheXagent-2-3b)
- [HuatuoGPT-Vision](https://huggingface.co/FreedomIntelligence/HuatuoGPT-Vision-7B)
- [MAIRA-2](https://huggingface.co/microsoft/maira-2)
- [Med-Flamingo-9B](https://huggingface.co/med-flamingo/med-flamingo)
- [LLaVA-Med-v1.5](https://huggingface.co/microsoft/llava-med-v1.5-mistral-7b)
- ...
</details>
> **Note**: If you are interested in other models that not list above, just email us, we are happy to consider and add to the list :)

### 3) üöÄ Recommand Framework
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- [VLLM](https://github.com/vllm-project/vllm)
- [SWIFT](https://github.com/maochaojie/swift)
- ..
> **Note**: You also free to use any framework, code you like :)

### üì¨ Contact Information

For any seminar-related questions, feel free to reach out **(remember to CC all of us)**:  
- üë®‚Äçüíª **Dr. Cosmin I. Bercea** ‚Äì [cosmin.bercea@tum.de](mailto:cosmin.bercea@tum.de)  
- üë©‚Äçüíº **Jun Li** ‚Äì [june.li@tum.de](mailto:june.li@tum.de)  
- üë©‚Äçüíº **Ha Young Kim** ‚Äì [hayoung.kim@tum.de](mailto:hayoung.kim@tum.de)

---

### üîó Group Homepage


üåê **[CompAI Lab ‚Äì Computational Imaging & AI in Medicine](https://compai-lab.github.io/)** 

